\documentclass[12pt, twocolumn]{article}
\usepackage{fullpage,enumerate,amsmath,amssymb,graphicx, float, lipsum}

\begin{document}
\twocolumn[
\begin{center}
{\Large CS224n Fall 2014 Programming Assignment 3}
\vspace{12pt}

SUNet ID: tzhang54, jiajihu

Name: Tong Zhang, Jiaji Hu
\vspace{12pt}
\end{center}]

\section{Naive Baselines}
\subsection{All Singleton}
In the all singleton naive baseline implementation, we simply iterated over all mentions and created a singleton entity for each mention. Each mention was marked coreferent with its own singleton entity.

\subsection{One Cluster}
In the one cluster naive baseline implementation, we created an entity at the very beginning. When we iterated over the mentions, we marked each mention as coreferent with the same entity.

\subsection{MUC Analysis}
The all singleton baseline got 100\% MUC precision but 0 MUC recall. On the other hand, the one cluster baseline got 100\% MUC recall but a lower MUC precision. This behavior made sense according to the definition of MUC. In all singleton baseline, each mention was clustered to it self, so there was never a wrong clustering. However, since the actual clustering did not include singletons, it was not able to capture any of the correct ones either, leading to a zero recall. One cluster baseline clustered all mentions together, so all pairs of mentions were connected by some edges, leading to a 100\% recall. The precision was low because some irrelevant mentions were clustered.

\section{Better Baseline}
Our baseline implementation was based on head words of the mentions. We decided to use head words because from the experiments we found that they were a very strong indicator of the clustering.
 In training, we iterated over all pairs of mentions that were clustered together and recorded their head words in a set. 
For test mentions, we checked if the pair of head words of two mentions had appeared in the training data. If the pair did appear in training, we would cluster the two mentions together.

\section{Rule Based}

\section{Classifier Based}
\subsection{Overview}
The classifier based system uses a set of positive and negative examples of coreferent pairs to train a maxent classifier. Every training example consists of a pair of mentions and a label of whether they are coreferent. For this part, we went along with only using the most recent coreferent mention as a positive example, and using only intervening mentions as negative examples. 
\subsection{Feature Engineering}
\subsubsection{Feature Design Process and\\ Error analysis}
For this part, we looked into features that we thought were relevant to classification. After we implement and test a feature, we look at the results, do error analysis and make modifications or extensions to make it better. Our feature design process was as follows:

To start, we only had the exact match feature. We were obviously bound to miss a lot of information using only this feature. 

Looking at the actual output of the system, things were as expected. For example, all mentions ``Hezbollah'' were put in the same cluster due to the exact match feature. However, the system was making mistakes even where the mentions were pretty much the same, but had just a little difference like an extra ``the''. For example: \texttt{{the strikes} -->  !{strikes}; !{these strikes}}

From that, we had the idea to use the head word of the mentions. This turned out to be a high impact feature. By adding an indicator feature, our score increase to MUC 0.70, B3 0.64. We also looked at the lemma of the headword, which was also a good feature, but it was overlapped with the headword feature.

With that feature, we saw that \texttt{{the strikes} -->  {strikes}; {these strikes}}, so that error case was solved. 

Next, we note that out system had 0.80+ precision but recall only around 0.6, which means we still don't have enough features to identify good coreference pairs. We thought that intuitively, coreferent mentions should be closer rather than further away. For example, there was a sentence: \texttt{since \{{the majority of strikers\}} are jobless and {\{they\}} have found...}, and we were not putting the two mentions in the same cluster. To try to solve this, we added mention distance as a feature.

However, it turned out that the mention distance alone did not improve performance. In retrospect, this may be because of the way we are creating our examples. Our negative examples are all ones that are closer than the positive example for that mention. Therefore, the information is rather minimal.

To improve on this, we observe that in many of these cases, one of the mentions is a pronoun. It makes sense that if one mention is a pronoun (such as ``they'' in our error example), mention distance may be more relevant. By pairing up the mention distance and the pronoun indicators, we got a slight increase in F1 score.

After that, we investigated the named entity type of the mentions. We experimented with the type or whether they're the same, but concluded that it did not have good impact because the named entity tagging is too simple to provide useful information.

At this point, we still had the problem of high precision and low recall -- we weren't extracting enough information.

We came across some interesting error cases that suggested that we could look at gender, speaker and plural compatibility:

\texttt{are \{{five members\}} , for example , so \{{they\}} will ... for \{{them\}} that \{{they\}} would not get}

For this example, our system clusters the two ``they''s, but misses the other two. However, we see that these are all plural.

\texttt{part of {\{Hezbollah\}} and {\{its}\} supporters ... strength of \{{this organization}\}}

For this example, all three were put in different clusters. However, we see that the plural and speaker features would help here.

Therefore, we added gender and plural features for the case of one mention being a name or noun and the other being a pronoun, or both mentions being pronouns. For the speaker, we only look at pronouns. Also, there is a decision of whether we want a positive indicator for the two mentions being compatible, or a negative indicator for the two mentions being incompatible. Intuitively, the positive feature is good for increasing recall, and the negative feature is good for improving precision.

As it turns out, both are good features, and their effects do not overlap. In addition, aggregating the three negative indicator features into one gave the same performance.

With the plural, gender and speaker features, our system was able to get right all four mentions in the first error example.
It was also able to correctly cluster \texttt{\{its\}} with \texttt{\{Hezbollah\}} for the second error example, so our features really had the expected effect on improving performance.

In the end, our system uses the following:
\begin{enumerate}[(1)]
\item Exact match (case insensitive)
\item Headword is same
\item Pair(Mention dist, Fixed is pronoun) 
\item Pair(Mention dist, Cand. is pronoun) 
\item Name-Pronoun gender incompatible
\item Name-Pronoun plural incompatible
\item Noun-Pronoun incompatible
\item Pronoun-Pronoun gender compatible
\item Pronoun-Pronoun plural compatible
\item Pronoun-Pronoun speaker compatible
\item Pronoun-Pronoun incompatible
\end{enumerate}
\subsubsection{Statistics}
Table~\ref{tab:features} shows some statistics of the classifier-based system on the development set using different feature combinations:
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Features & MUC F1 & B3 F1\\\hline
1 & 0.642 & 0.601 \\\hline
1,2 & 0.707 & 0.645 \\\hline
1,2,3,4,5-7 & 0.755 & 0.679\\\hline
1,2,3,4,8-10 & 0.740 & 0.678\\\hline
1,2,3,4,11 & 0.743 & 0.684 \\\hline
All & 0.792 & 0.721 \\\hline
\end{tabular}
\end{center}
\caption{Results over feature combinations}
\label{tab:features}
\end{table}
\subsubsection{Final errors and future\\ improvements}
From the output of our system, we can still see a few errors that can potentially be fixed.

For example, \texttt{\{this organization\} --> !\{Hezbollah\}; !\{Hezbollah\}; !\{its\}; !\{Hezbollah\} ...} shows that we are just not getting that ``Hezbollah'' is an organization -- something that NER should be able to pick up. If we incorporated named entity tagging as a new feature, mistakes like this should be avoided, and we could get substantially improved recall.

A similar error case that would benefit from knowing named entity types is \texttt{\{Yemen 's President\}} with \texttt{\{him\}; \{he\}; \{his\};}.

Also, we still miss a lot of \texttt{\{this\}} references, so incorporating knowledge from the parse tree will definitely help in those situations, which is something we can improve on.

Finally, there are some errors that are quite understandable. For example, on the test set, our system was unable to realize that ``The USS Cole'' was a carrier, which was a ship. Things like this would benefit from world knowledge that the system obviously doesn't have. Incorporating this knowledge seems quite difficult for our framework.
\section{Results}
The results for this assignment are shown in Table~\ref{tab:results}. As you can see, ...............
\begin{table}[b]
\begin{minipage}{\textwidth}
\centering
\begin{tabular}{l|c c c|c c c}
\hline
& & MUC & & & B3 & \\
Algorithm & Prec & Recall & F1 & Prec & Recall & F1 \\\hline
Rule-based & 0 & 0 & 0 & 0 & 0 & 0\\\hline  
Classifier-based & 0 & 0 & 0 & 0 & 0 & 0\\\hline
\end{tabular}
\caption{Results for all systems}\label{tab:results}
\end{minipage}
\end{table}


\section{Extra Credit}

\section{Collaboration} 

\end{document}