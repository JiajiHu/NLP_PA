\documentclass[12pt, twocolumn]{article}
\usepackage{fullpage,enumerate,amsmath,amssymb,graphicx, float, lipsum}

\begin{document}
\twocolumn[
\begin{center}
{\Large CS224n Fall 2014 Programming Assignment 1}
\vspace{12pt}

SUNet ID: tzhang54, jiajihu

Name: Tong Zhang, Jiaji Hu
\vspace{12pt}
\end{center}]

\section{Naive Baselines}
\subsection{All Singleton}
In the all singleton naive baseline implementation, we simply iterated over all mentions and created a singleton entity for each mention. Each mention was marked coreferent with its own singleton entity.

\subsection{One Cluster}
In the one cluster naive baseline implementation, we created an entity at the very beginning. When we iterated over the mentions, we marked each mention as coreferent with the same entity.

\subsection{MUC Analysis}
The all singleton baseline got 100\% MUC precision but 0 MUC recall. On the other hand, the one cluster baseline got 100\% MUC recall but a lower MUC precision. This behavior made sense according to the definition of MUC. In all singleton baseline, each mention was clustered to it self, so there was never a wrong clustering. However, since the actual clustering did not include singletons, it was not able to capture any of the correct ones either, leading to a zero recall. One cluster baseline clustered all mentions together, so all pairs of mentions were connected by some edges, leading to a 100\% recall. The precision was low because some irrelevant mentions were clustered.

\section{Better Baseline}
Our baseline implementation was based on head words of the mentions. We decided to use head words because from the experiments we found that they were a very strong indicator of the clustering.
 In training, we iterated over all pairs of mentions that were clustered together and recorded their head words in a set. 
For test mentions, we checked if the pair of head words of two mentions had appeared in the training data. If the pair did appear in training, we would cluster the two mentions together.

\section{Rule Based}

\section{Classifier Based}
\subsection{Overview}

\subsection{Feature Engineering}
\subsubsection{Feature Design Process}

In the end, our system uses following features:
\begin{enumerate}[(1)]
\item 
\end{enumerate}
\subsubsection{Statistics}
Table~\ref{tab:features} shows some statistics of the classifier-based system on the development set using different feature combinations:
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Features & MUC F1 & B3 F1\\\hline
\end{tabular}
\end{center}
\caption{Results over feature combinations}
\label{tab:features}
\end{table}
\subsection{Error Analysis}

\section{Results}
The results for this assignment are shown in Table~\ref{tab:results}. As you can see, ...............
\begin{table}[b]
\begin{minipage}{\textwidth}
\centering
\begin{tabular}{l|c c c|c c c}
\hline
& & MUC & & & B3 & \\
Algorithm & Prec & Recall & F1 & Prec & Recall & F1 \\\hline
Rule-based & 0 & 0 & 0 & 0 & 0 & 0\\\hline  
Classifier-based & 0 & 0 & 0 & 0 & 0 & 0\\\hline
\end{tabular}
\caption{Results for all systems}\label{tab:results}
\end{minipage}
\end{table}


\section{Extra Credit}

\section{Collaboration} 

\end{document}