\documentclass[12pt, twocolumn]{article}
\usepackage{fullpage,enumerate,amsmath,amssymb,graphicx, float, lipsum}

\begin{document}
\twocolumn[
\begin{center}
{\Large CS224n Fall 2014 Programming Assignment 3}
\vspace{12pt}

SUNet ID: tzhang54, jiajihu

Name: Tong Zhang, Jiaji Hu
\vspace{12pt}
\end{center}]

\section{Naive Baselines}
\subsection{All Singleton}
In the all singleton naive baseline implementation, we simply iterated over all mentions and created a singleton entity for each mention. Each mention was marked coreferent with its own singleton entity.

\subsection{One Cluster}
In the one cluster naive baseline implementation, we created an entity at the very beginning. When we iterated over the mentions, we marked each mention as coreferent with the same entity.

\subsection{MUC Analysis}
The all singleton baseline got 100\% MUC precision but 0 MUC recall. On the other hand, the one cluster baseline got 100\% MUC recall but a lower MUC precision. This behavior made sense according to the definition of MUC. In all singleton baseline, each mention was clustered to it self, so there was never a wrong clustering. However, since the actual clustering did not include singletons, it was not able to capture any of the correct ones either, leading to a zero recall. One cluster baseline clustered all mentions together, so all pairs of mentions were connected by some edges, leading to a 100\% recall. The precision was low because some irrelevant mentions were clustered.

\section{Better Baseline}
Our baseline implementation was based on head words of the mentions. We decided to use head words because from the experiments we found that they were a very strong indicator of the clustering.
 In training, we iterated over all pairs of mentions that were clustered together and recorded their head words in a set. 
For test mentions, we checked if the pair of head words of two mentions had appeared in the training data. If the pair did appear in training, we would cluster the two mentions together.

\section{Rule Based}

\section{Classifier Based}
\subsection{Overview}
The classifier based system uses a set of positive and negative examples of coreferent pairs to train a maxent classifier. Every training example consists of a pair of mentions and a label of whether they are coreferent. For this part, we went along with only using the most recent coreferent mention as a positive example, and using only intervening mentions as negative examples. 
\subsection{Feature Engineering}
\subsubsection{Feature Design Process}
For this part, we looked into features that we thought were relevant to classification. After we implement and test a feature, we look at the results and make modifications or extensions to make it better. Our feature design process is as follows:

First, we looked at the distance between the candidate mentions. Intuitively, coreferent mentions should be closer rather than further away. However, it turned out that neither the distance in terms of mentions or in terms of sentences improved the performance. In retrospect, this may be because of the way we are creating our examples. Our negative examples are all ones that are closer than the positive example for that mention. Therefore, the information is rather minimal.

Next, we moved on to pronouns. Intuitively this alone does not bring much information, and the results agreed with our expectation. However, we came up with the idea that if one of the mentions is a pronoun, it may be more interesting to see if they are close together. By pairing up the mention distance and the pronoun indicators, we got a slight increase in F1 score.

After that, as suggested by the guidelines, we investigated the named entity type of the fixed and candidate mentions. After experimenting with the type, or whether they are the same, we concluded that this was not an impactful feature.

Then, we looked into using the head word of the mentions. This turned out to be a high impact feature. By adding an indicator to see if the head words were a match, we immediately got a significant increase in score. We also looked at the lemma of the headword, which was also a good feature, but it was overlapped with the headword feature.

Finally, we looked at gender, speaker and plural compatibility. For gender and plural, we looked at the case of one mention being a name or noun and the other being a pronoun, or both mentions being pronouns. For the speaker, we only look at pronouns. Also, there is a decision of whether we want a positive indicator for the two mentions being compatible, or a negative indicator for the two mentions being incompatible. As it turns out, both are good features, and their effects do not overlap. In addition, aggregating the three negative indicator features into one feature gave the same performance.

In the end, our system uses the following:
\begin{enumerate}[(1)]
\item Exact match (case insensitive)
\item Pair(Mention dist, Fixed is pronoun) 
\item Pair(Mention dist, Cand. is pronoun) 
\item Name-Pronoun gender incompatible
\item Name-Pronoun plural incompatible
\item Noun-Pronoun incompatible
\item Pronoun-Pronoun gender compatible
\item Pronoun-Pronoun plural compatible
\item Pronoun-Pronoun speaker compatible
\item Pronoun-Pronoun incompatible
\item Headword is same
\end{enumerate}
\subsubsection{Statistics}
Table~\ref{tab:features} shows some statistics of the classifier-based system on the development set using different feature combinations:
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Features & MUC F1 & B3 F1\\\hline
1 & 0.642 & 0.601 \\\hline
1,2,3,4,5,6 & 0.690 & 0.639\\\hline
1,2,3,7,8,9 & 0.672 & 0.636\\\hline
1,2,3,10 & 0.678 & 0.641 \\\hline
1,2,3,4-10 & 0.731 & 0.688 \\\hline
1,11 & 0.707 & 0.645 \\\hline
All & 0.792 & 0.721 \\\hline
\end{tabular}
\end{center}
\caption{Results over feature combinations}
\label{tab:features}
\end{table}
\subsection{Error Analysis}
characterizing systematic mistakes the system makes. You should describe how these features
would be fixed, either with more features (and if so, whatwould they be?), or note that they cannot be easily captured
in our framework.
\section{Results}
The results for this assignment are shown in Table~\ref{tab:results}. As you can see, ...............
\begin{table}[b]
\begin{minipage}{\textwidth}
\centering
\begin{tabular}{l|c c c|c c c}
\hline
& & MUC & & & B3 & \\
Algorithm & Prec & Recall & F1 & Prec & Recall & F1 \\\hline
Rule-based & 0 & 0 & 0 & 0 & 0 & 0\\\hline  
Classifier-based & 0 & 0 & 0 & 0 & 0 & 0\\\hline
\end{tabular}
\caption{Results for all systems}\label{tab:results}
\end{minipage}
\end{table}


\section{Extra Credit}

\section{Collaboration} 

\end{document}